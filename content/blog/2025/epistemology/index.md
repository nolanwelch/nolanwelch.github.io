---
title: "The Threat of Privatized Epistemology"
description: "Silicon Valley is making its proprietary, hallucinating, parroting homunculi the sole arbiters of truth. What could go wrong?"
author: "Nolan Welch"
date: 2025-05-22T04:14:54-08:00
draft: true
tags: ["LLMs", "AI", "Elon Musk", "Grok"]
params:
  ShowReadingTime: true
  ShowShareButtons: true
  ShareButtons: ["x", "linkedin", "reddit", "facebook"]
---

xAI's Grok [made](https://www.rollingstone.com/culture/culture-news/elon-musk-x-grok-white-genocide-holocaust-1235341267/) [headlines](https://www.readtpa.com/p/the-grok-white-genocide-incident) [last](https://www.cnn.com/2025/05/20/business/grok-genocide-ai-nightcap) [week](https://techcrunch.com/2025/05/14/grok-is-unpromptedly-telling-x-users-about-south-african-genocide/) when the bot repeatedly---and seemingly inexplicably---made reference to the controversial ["kill the Boer"](https://en.wikipedia.org/wiki/Dubul'_ibhunu) political slogan in response to entirely unrelated user prompts, such as

Setting aside the question of whether or not .

The concern I hold goes beyond the use of large language models as tools to spread propaganda. In an age when a vast majority of digital (and all) communication takes place within the confines of vast social media networks whose owners have an "iffy", at best, relationship with the concept of freedom of speech, deciding the truth of a claim isn't just grounds for its rejection; it's grounds for removal.

Donald Trump's EO on ["Preventing Woke AI in the Federal Government"](https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/)

# The problems with AI as general reasoners

As someone who studied Computer Science and Linguistics in undergrad, I have a lot of opinions on the idea of "General Artificial Intelligence" and the idea that the path towards it is the development of large-scale text-based models---far too many to comfortably cover in this post.[^1] The long and short of it is: the real world is an incredibly complex thing, and languages are simply not built to serve as a proper model of it. Language facilitates communication between humans on the basis of shared cultural context, and that's it.

This may seem like an innocuous point to raise

# Novel approaches to fact-checking

# Why I'm still worried

1. Proprietary. We have little to no insight into the training data or methods on how these models are produced.
2. Parroting. Models have ABSOLUTELY NO ability to reason, and will spit out whatever junk is most statistically likely.
3. Hallucinating. There is no _a priori_ guarantee on semantic or logical correctness in LLM output; the machine outputs what it deems to be sufficiently statistically likely, with no other metric for determining whether it "makes sense".

[^1]: Fret not, dear reader: my hot take on this topic will be forthcoming.
